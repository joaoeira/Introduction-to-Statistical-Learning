lm.fit=lm(Sales~Population+Urban+US+Population*US)
summary(lm.fit)
lm.fit=lm(Sales~Population+Urban+US+Urban*US)
summary(lm.fit)
lm.fit=lm(Sales~Population+Urban+US)
summary(lm.fit)
?Carseats
lm.fit=lm(Sales~Population+Urban+US+Income+Income*US)
summary(lm.fit)
lm.fit=lm(Sales~Population+Urban+US)
summary(lm.fit)
\end{matrix}\right.$$
lm.fit=lm(Sales~US)
summary(lm.fit)
lm.fit1=lm(Sales~US)
summary(lm.fit1)
lm.fit=lm(Sales~Population+Urban+US)
summary(lm.fit)
par(mfrow=c(2,2))
plot(lm.fit1)
predict(lm.fit1, interval = "confidence")
confint(lm.fit1,level=0.95)
plot(hatvalues(lm.fit))
rm(list=ls())
set.seed(1)
x=rnorm(100)
y=2*x+rnorm(100)
lm.fit <- lm(y~x)
summary(lm.fit)
lm.fit <- lm(y~x-Intercept)
lm.fit<-update(lm.fit,-Intercept)
lm.fit<-update(lm.fit,~.-1)
summary(lm.fit)
lm.fit <- lm(y~x)
summary(lm.fit)
lm.fit<-update(lm.fit,~.-2) #update regression to remove the first term
lm.fit<-update(lm.fit,~.-0) #update regression to remove the first term
summary(lm.fit)
lm.fit<-update(lm.fit,~.-1) #update regression to remove the first term
summary(lm.fit)
lm.fit<-update(lm.fit,~.-1) #update regression to remove the first term
summary(lm.fit)
lm.fit<-update(lm.fit,~.-1) #update regression to remove the first term
summary(lm.fit)
lm.fit<-update(lm.fit,~.-2) #update regression to remove the first term
lm.fit<-update(lm.fit,~.-1) #update regression to remove the first term
lm.fit <- lm(y~x)
lm.fit<-update(lm.fit,~.-2) #why does this work but -2 doesnt?
lm.fit<-update(lm.fit,~.-1) #why does this work but -2 doesnt?
summary(lm.fit)
lm.fit <- lm(y~x+0)
summary(lm.fit)
plot(lm.fit)
plot(lm.fit)
x <- rnorm(100)
y <- 2*x+rnorm(100)
lm.fit <- lm(y~x+0)
summary(lm.fit)
plot(lm.fit)
lm.fit <- lm(x~y+0)
summary(lm.fit)
plot(lm.fit)
rm(list=lm())
rm(list=ls())
x = c(1:100)
x
y=c(2:50,2)
y
y=c(2:50,by =0.5)
y
range(y)
length(y)
length(x)
y=c(2,50,by =0.5)
y=c(2,50,by =0.5)
length(x)
length(y)
y=seq(2,50,by =0.5)
length(y)
y
y=seq(0,50,by =0.5)
plot(x,y)
length(y)
y=seq(0.5,50,by =0.5)
plot(x,y)
par(mfrow=c(1,1))
plot(x,y)
plot(y,x)
lm.fit=lm(x~y)
lm.fit1=lm(y~x)
summary(lm.fit)
summary(lm.fit1)
lm.fit1=lm(y~x-0)
summary(lm.fit1)
lm.fit1=lm(y~x+0)
summary(lm.fit1)
lm.fit=lm(x~y+0)
summary(lm.fit)
x=c(1:100)
y=seq(100:200)
plot(x,y)
length(y)
length(y)
y
x
y=seq(100:200)
y=seq(100,200)
y=seq(100,200)
y
plot(x,y)
length(y)
x
y=seq(101,200)
plot(x,y)
plot(y,x)
lm.fit=lm(x~y+0)
summary(lm.fit)
lm.fit1=lm(y~x+0)
summary(lm.fit1)
x<-c(1:100)
y<-x
lm.fit<-lm(x~y+0)
summary(lm.fit)
lm.fit1<-lm(y~x+0)
summary(lm.fit1)
rm(list=ls())
x <- rnorm()
x <- rnorm(100)
hist(x)
x <- rnorm(100,mean=0,sd=1)
hist(x)
hist(x,bn=1)
hist(x,bin=1)
hist(x,bin=2)
hist(x)
eps<-rnorm(100,mean=0,sd=0.25)
hist(eps)
eps<-rnorm(100,mean=0,sd=0.25)
y<--1+0.5*x+eps
hist(eps)
length(y)
plot(x,y)
lm.fit=lm(y~x)
summary(lm.fit)
abline(lm.fit)
abline(lm.fit,legend("lm.fit""))
s
abline(lm.fit,legend("lm.fit"))
abline(lm.fit)
abline(y)
plot(y)
plot(x,y)
lm.fit=lm(y~x)
summary(lm.fit)
abline(lm.fit)
plot(-1+0.5*x+eps,x)
plot(x,y)
lm.fit=lm(y~x)
summary(lm.fit)
abline(lm.fit)
a=c(-5:5)
plot(-1+0.5*c+eps,x)
curve(-1+0.5*x)
plot(x,y)
abline(curve(-1+0.5*x))
curve(-1+0.5*x)
plot(x,y)
#e)
lm.fit=lm(y~x)
summary(lm.fit)
abline(lm.fit)
curve(-1+0.5*x,add=TRUE)
curve(-1+0.5*x,add=TRUE,col="red")
curve(-1+0.5*x,add=TRUE,col="red",lwd=3)
abline(lm.fit,lwd=3)
abline(lm.fit,lwd=3,col="blue")
legend(1,-1.5, c("Population","Regression"), lty=c(1,1),lwd=c(2.5,2.5),col=c("blue","red"))
legend(1,-1, c("Population","Regression"), lty=c(1,1),lwd=c(2.5,2.5),col=c("blue","red"))
legend(1,1, c("Population","Regression"), lty=c(1,1),lwd=c(2.5,2.5),col=c("blue","red"))
plot(x,y)
lm.fit=lm(y~x)
summary(lm.fit)
abline(lm.fit,lwd=3,col="blue")
curve(-1+0.5*x,add=TRUE,col="red",lwd=3)
legend(1,1, c("Population","Regression"), lty=c(1,1),lwd=c(2.5,2.5),col=c("blue","red"))
plot(x,y)
legend(1,-1, c("Population","Regression"), lty=c(1,1),lwd=c(2.5,2.5),col=c("blue","red"))
plot(x,y)
legend(0,-1, c("Population","Regression"), lty=c(1,1),lwd=c(2.5,2.5),col=c("blue","red"))
legend(0.5,-1, c("Population","Regression"), lty=c(1,1),lwd=c(2.5,2.5),col=c("blue","red"))
legend(0.1,-1, c("Population","Regression"), lty=c(1,1),lwd=c(2.5,2.5),col=c("blue","red"))
legend(1,-1, c("Population","Regression"), lty=c(1,1),lwd=c(2.5,2.5),col=c("blue","red"))
legend(0.9,-1, c("Population","Regression"), lty=c(1,1),lwd=c(2.5,2.5),col=c("blue","red"))
legend(0.7,-1, c("Population","Regression"), lty=c(1,1),lwd=c(2.5,2.5),col=c("blue","red"))
plot(x,y)
legend(0.7,-1, c("Population","Regression"), lty=c(1,1),lwd=c(2.5,2.5),col=c("blue","red"))
legend(0.7,0, c("Population","Regression"), lty=c(1,1),lwd=c(2.5,2.5),col=c("blue","red"))
plot(x,y)
#e)
lm.fit=lm(y~x)
summary(lm.fit)
abline(lm.fit,lwd=3,col="blue")
curve(-1+0.5*x,add=TRUE,col="red",lwd=3)
legend(0.7,0, c("Population","Regression"), lty=c(1,1),lwd=c(2.5,2.5),col=c("blue","red"))
legend(0.7,-1, c("Population","Regression"), lty=c(1,1),lwd=c(2.5,2.5),col=c("blue","red"))
plot(x,y)
#e)
lm.fit=lm(y~x)
summary(lm.fit)
abline(lm.fit,lwd=3,col="blue")
curve(-1+0.5*x,add=TRUE,col="red",lwd=3)
legend(0.7,-1, c("Population","Regression"), lty=c(1,1),lwd=c(2.5,2.5),col=c("blue","red"))
plot(x,y)
lm.fit=lm(y~x)
summary(lm.fit)
abline(lm.fit,lwd=3,col="blue")
legend(0.7,-1, c("Population","Regression"), lty=c(1,1),lwd=c(2.5,2.5),col=c("blue","red"))
legend(0.7,-1, c("Population","Regression"), lty=c(1,1),lwd=c(2.5,2.5),col=c("red","blue"))
abline(lm.fit,lwd=3,col="blue")
curve(-1+0.5*x,add=TRUE,col="red",lwd=3)
legend(0.7,-1, c("Population","Regression"), lty=c(1,1),lwd=c(2.5,2.5),col=c("red","blue"))
x<-rnorm(1000,mean=0,sd=1)
#b)
eps<-rnorm(1000,mean=0,sd=0.25)
#c)
y<--1+0.5*x+eps
length(y)
#beta0 = -1
#beta1 = 0.5
#d)
plot(x,y)
#e)
lm.fit=lm(y~x)
summary(lm.fit)
abline(lm.fit,lwd=3,col="blue")
#because of n=100, if n=infty would be the same
#f)
curve(-1+0.5*x,add=TRUE,col="red",lwd=3)
legend(0.7,-1, c("Population","Regression"), lty=c(1,1),lwd=c(2.5,2.5),col=c("red","blue"))
x<-rnorm(10000,mean=0,sd=1)
#b)
eps<-rnorm(10000,mean=0,sd=0.25)
#c)
y<--1+0.5*x+eps
length(y)
#beta0 = -1
#beta1 = 0.5
#d)
plot(x,y)
#e)
lm.fit=lm(y~x)
summary(lm.fit)
abline(lm.fit,lwd=3,col="blue")
#because of n=100, if n=infty would be the same
#f)
curve(-1+0.5*x,add=TRUE,col="red",lwd=3)
legend(0.7,-1, c("Population","Regression"), lty=c(1,1),lwd=c(2.5,2.5),col=c("red","blue"))
x<-rnorm(100,mean=0,sd=1)
#b)
eps<-rnorm(100,mean=0,sd=0.25)
#c)
y<--1+0.5*x+eps
length(y)
#beta0 = -1
#beta1 = 0.5
#d)
plot(x,y)
#e)
lm.fit=lm(y~x)
summary(lm.fit)
abline(lm.fit,lwd=3,col="blue")
#because of n=100, if n=infty would be the same
#f)
curve(-1+0.5*x,add=TRUE,col="red",lwd=3)
legend(0.7,-1, c("Population","Regression"), lty=c(1,1),lwd=c(2.5,2.5),col=c("red","blue"))
lm.fit1<-lm(y~x+I(x^2))
summary(lm.fit1)
set.seed(1)
rm(list=ls())
#a)
set.seed(1)
x1<-runif(100)
x2=0.5*x1*norm(100)/10
y=2+2*x1+0.3*x2+rnorm(100)
x2=0.5*x1*norm(100)/10
x2=0.5*x1*rnorm(100)/10
y=2+2*x1+0.3*x2+rnorm(100)
cor(x1,x2)
plot(x1,x2)
lm.fit<-lm(y~x1+x2)
summary(lm.fit)
par(mfrow=c(2,2))
plot(lm)
plot(lm.fit)
lm.fit<-(y~x1)
summary(lm.fit)
summary(lm.fit)
lm.fit<-(y~x1)
summary(lm.fit)
# Chunk 1
library("ISLR")
attach(Auto)
# Chunk 2
#a)
lm.fit <- lm(mpg~horsepower)
summary(lm.fit)
# Chunk 3
predict(lm.fit,data.frame(horsepower=98), interval = "confidence")
predict(lm.fit,data.frame(horsepower=98), interval = "prediction")
# Chunk 4
#b)
par(mfrow=c(1,1))
plot(horsepower,mpg)
abline(lm.fit, lwd=1) #protip: abline plots ON TOP of another plot, you can't just call abline
# Chunk 5
#c)
par(mfrow=c(2,2))
plot(lm.fit)
lm.fit1 <- lm(mpg~I(log(horsepower)))
par(mfrow=c(1,1))
plot(log(horsepower),mpg)
abline(lm.fit1)
par(mfrow=c(2,2))
plot(lm.fit1)
#the residuals seem to follow less of a pattern when its log(horsepower)
#the fit also seems much better
# Chunk 6
rm(list=ls())
attach(Auto)
?Auto
# Chunk 7
#a)
pairs(Auto)
# Chunk 8
#b)
cor(Auto[-9])
# Chunk 9
#c)
lm.fit <- lm(mpg~.-name, data=Auto)
summary(lm.fit)
#i) and ii)
#there is for some of them: the intercept,displacement, weight,year and origin
#iii)
#the year coefficient suggests that as time has passed mpg has increased
#meaning that manufacturers have been improving their engines so that
#miles per gallon has increased, i.e. better efficiency
# Chunk 10
#d)
par(mfrow=c(2,2))
plot(lm.fit)
#there are some outliers, considering the leverage
par(mfrow=c(1,1))
plot(hatvalues(lm.fit))
which.max(hatvalues(lm.fit)) #14
# Chunk 11
#e)
lm.fit1 <- lm(mpg~.-name +year*horsepower, data=Auto)
summary(lm.fit1)
par(mfrow=c(2,2))
plot(lm.fit1)
#some heteroscedasticity I think
lm.fit1 <- lm(mpg~.-name +horsepower*displacement, data=Auto)
summary(lm.fit1)
par(mfrow=c(2,2))
plot(lm.fit1)
#why does suddenly everything seem to be somewhat significant except the intercept?
lm.fit1 <- lm(mpg~.-name +horsepower*acceleration, data=Auto)
summary(lm.fit1)
par(mfrow=c(2,2))
plot(lm.fit1)
# Chunk 12
#f)
lm.fit2<-lm(log(mpg)~.-name, data=Auto)
summary(lm.fit2)
plot(lm.fit2)#less of a pattern in the residuals
lm.fit3<-lm(log(mpg)~.-name -horsepower -displacement -acceleration -cylinders, data=Auto)
summary(lm.fit3)
plot(lm.fit3)
#doesnt lose much in terms of R2 and plots seem much better
lm.fit4<-lm(I(mpg^2)~.-name -horsepower -displacement -acceleration -cylinders, data=Auto)
summary(lm.fit4)
plot(lm.fit4)
#worse, and some error "operator is invalid for atomic vectors"
# Chunk 13
rm(list=ls())
attach(Carseats)
?Carseats
# Chunk 14
#a)
lm.fit=lm(Sales~Population+Urban+US)
summary(lm.fit)
#b)
#sales do better in the US? also, sales would still exist even if not in the US (no population???)
# Chunk 15
#e)
lm.fit1=lm(Sales~US)
summary(lm.fit1)
#f) not very well, R2 ~0.03
# Chunk 16
#g)
confint(lm.fit1,level=0.95)
# Chunk 17
#h)
plot(hatvalues(lm.fit))
#No? doesnt seem like there is
# Chunk 18
rm(list=ls())
set.seed(1)
x <- rnorm(100)
y <- 2*x+rnorm(100)
# Chunk 19
#a)
lm.fit <- lm(y~x+0)
summary(lm.fit)
plot(lm.fit)
# Chunk 20
#b)
lm.fit <- lm(x~y+0)
summary(lm.fit)
plot(lm.fit)
#c) boring
#d) see notes
# Chunk 21
rm(list=ls())
par(mfrow=c(1,1))
# Chunk 22
#a) beta = 1
#b)
x <- c(1:100)
y<-seq(0.5,50,by =0.5)
lm.fit<-lm(x~y+0)
summary(lm.fit)
lm.fit1<-lm(y~x+0)
summary(lm.fit1)
# Chunk 23
#c)
x<-c(1:100)
y<-x
lm.fit<-lm(x~y+0)
summary(lm.fit)
lm.fit1<-lm(y~x+0)
summary(lm.fit1)
# Chunk 24
rm(list=ls())
# Chunk 25
#a)
x<-rnorm(100,mean=0,sd=1)
#b)
eps<-rnorm(100,mean=0,sd=0.25)
#c)
y<--1+0.5*x+eps
length(y)
#beta0 = -1
#beta1 = 0.5
#d)
plot(x,y)
#e)
lm.fit<-lm(y~x)
summary(lm.fit)
abline(lm.fit,lwd=3,col="blue")
#because of n=100, if n=infty would be the same
#f)
curve(-1+0.5*x,add=TRUE,col="red",lwd=3)
legend(0.7,-1, c("Population","Regression"), lty=c(1,1),lwd=c(2.5,2.5),col=c("red","blue"))
#g)
lm.fit1<-lm(y~x+I(x^2))
summary(lm.fit1)
#not significant
#h)
#less noise allows for "truer" fit using less observations
#i)
#)more noise needs more observations to reach a good fit
# Chunk 26
rm(list=ls())
#a)
set.seed(1)
x1=runif(100)
x2=0.5*x1*rnorm(100)/10
y=2+2*x1+0.3*x2+rnorm(100)
#b)
cor(x1,x2)
plot(x1,x2)
#c)
lm.fit<-lm(y~x1+x2)
summary(lm.fit)
par(mfrow=c(2,2))
plot(lm.fit)
#d)
lm.fit<-(y~x1)
summary(lm.fit)
rm(list=ls())
set.seed(1)
x1=runif(100)
x2=0.5*x1*rnorm(100)/10
y=2+2*x1+0.3*x2+rnorm(100)
#b)
cor(x1,x2)
plot(x1,x2)
lm.fit<-(y~x1)
summary(lm.fit)
lm.fit<-lm(y~x1)
summary(lm.fit)
lm.fit<-lm(y~x2)
summary(lm.fit)
lm.fit2<-lm(log(mpg)~.-name, data=Auto)
lm.fit2<-lm(log(mpg)~.-name, data=Auto)
summary(lm.fit2)
plot(lm.fit2)#less of a pattern in the residuals
lm.fit3<-lm(log(mpg)~.-name -horsepower -displacement -acceleration -cylinders, data=Auto)
summary(lm.fit3)
plot(lm.fit3)
#doesnt lose much in terms of R2 and plots seem much better
