---
title: "Chapter 6 - Linear Model Selection and Regularization"
output:
  html_document: default
  html_notebook: default
---

Why might we want to use another fitting procedure instead of least squares? As we will see, alternative fitting procedures can yield better *prediction accuracy* and *model interpretability*.

 -*Prediction Accuracy*: If the relationship between the response and the predictors is approximately linear, the least square estimates will have low bias. If $n \gg p$ then the least square estimates will also tend to have low variance, hence performing well on test observations. However, if $n$ is not so much larger than $p$, that is, the number of observations is not much larger than the number of predictors, or variables, there can be a lot of variability in the least squares fit, resulting in overfitting and therefore poor predictions. Also, if $p>n$, there ceases to exist a unique least squares coefficient estimate. By constraining or shrinking the estimated coefficients, we can often substantially reduce the variance at the cost of a negligible increase in bias. This can lead to substantial improvements in the accuracy with which we can predict the response for observations not used in model training.
 
 -*Model Interpretability*: It is often the case that some or many of the variables used in a multiple regression model are in fact not associated with the response. Including such irrelevant variables leads to unnec- essary complexity in the resulting model. By removing these variables — that is, by setting the corresponding coefficient estimates to zero — we can obtain a model that is more easily interpreted. Now least squares is extremely unlikely to yield any coefficient estimates that are exactly zero. In this chapter, we see some approaches for automatically performing feature selection or variable selection — that is, for excluding irrelevant variables from a multiple regression model
 
#Subset Selection
 
##Best Subset Selection

**Algorithm: Best Subset Selection**
1. Let $M_0$ denote the *null model*. which contains no predictors. This model simply predicts the sample mean for each observation.
2. For $k=1,2,...,p$:
  a) Fit all $\binom{p}{k}$ models that contain exactly $k$ predictors
  b) Pick the best among these $\binom{p}{k}$ models, and call it $M_k$. Here *best* is defined as having the smallest RSS, orquivalently, largest $R^2$
3. Select a single best model from among $M_0,...,M_p$ using cross-validated prediction error, $C_p$ (AIC),BIC, or adjusted $R^2$.


We must need be careful in the third step because RSS decreases monotonically as $p$ increases, and $R^2$ increases. Therefore, we use cross-validated prediction error, $C_p$, BIC, or adjusted $R^2$ to select among the models.

While best subset selection is a simple and conceptually appealing approach, it suffers from computational limitations. The number of possible models that must be considered grows rapidly as $p$ increases. In general, there are $2^p$ models that involve subsets of $p$ predictors.

##Stepwise Selection

For computational reasons, best subset selection cannot be applied with very large $p$. Best subset selection may also suffer from statistical problems when $p$ is large. The larger the search space, the higher the chance of finding models that look good on the training data, even though they might not have any predictive power on future data. Thus an enormous search space can lead to overfitting and high variance of the coefficient estimates. For both of these reasons, stepwise methods, which explore a far more restricted set of models, are attractive alternatives to best subset selection. 

**Forward Stepwise Selection**

Forward  stepwise selection begins with a model containing no predictors, and then adds predictors to the model, one-at-a-time, until all of the predictors are in the model. In particular, at each step the variable that gives the greatest *additional* improvement to the fit is added to the model. 

Unlike best subset selection, which involved fitting $2^p$ models, forward stepwise selection involves fitting one null model, along with $p-k$ in the $k$th iteration. This amounts to a total of $1+p(p+1)/2$ models. 

**Algorithm: Forward Stepwise Selection**
1. Let $M_0$ denote the *null* model, which contains no predictors.
2. For $k=0,...,p-1$:
  a) Consider all $p-k$ models that augment the predictors in $M_k$ with one additional predictor
  b) Choose the *best* among these $p-k$ models, and call it $M_k{k+1}$. Here *best* is defined as having smallest RSS or highest $R^2$
3. Select a single best model from among $M_0,...,M_p$ using cross-validated prediction error, $C_p (AIC)$, BIC, or adjusted $R^2$.

Though forward stepwise tends to do well in practice, it is not guaranteed to find the best possible model out of all $2^p$ models containing subsets of the $p$ predictors. FOr isntance, suppose that in a given data set with $p=3$ predictors, the best possible one-variable model contains $X_1$, and the best possible two-variable model contains $X_2$ and $X_3$. THen forward stepwise selection will fail to select the best possible two-variable model, because $M_1$ will contain $X_1$, so $M_2$ must also contain $X_1$ together with one additional variable.

## Backward Stepwise Selection

Unlike forward stepwise selection, backward selection begins with the full least squares model containing all $p$ predictors, and then iteratively removes the least useful predictor, one-at-a-time.

**Algorithm: Backward Stepwise Selection**
1. Let $M_p$ denote the *full* model, which contains all $p$ predictors.
2. For $k=p,p-1,...,1$:
  a) COnsider all $k$ models that contain all but one of the predictors in $M_k$, for a total of $k-1$ predictors.
  b) Choose the *best* among these $k$ models, and call it $M_{k-1}$. Here *best* is defined as having smallest RSS or highest $R^2$
3. Select a single best model from among $M_0,...,M_p$ using cross-validated prediction error, $C_p(AIC)$,BIC, or adjusted $R^2$

Like forward stepwise selection, backward selection isnot guaranteed to yield the *best* model containing a subset of the *p* predictors.

## Hybrid Approaches

The best subset, forward stepwise, and backward stepwise selection approaches generally give similar but not identical models. As another alternative, hybrid versions of forward and backward stepwise selection are available, in which variables are added to the model sequentially, in analogy to forward selection. However, after adding each new variable, the method may also remove any variables that no longer provide an improvement in the model fit. Such an approach attempts to more closely mimic best subset selection while retaining the computational advantages of forward and backward stepwise selection.

# Choosing the Optimal Model

In order to select the best model with respect to test error, we need to estimate this test error. There are two common approacheS:

1. We can indirectly estimate test error bu making an adjustment to the training error to account for the bias due to overfitting
2. We can *directly* estimate the test error, using either a validation set approach or a cross-validation approach, as discussed in Chapter 5.

##$C_p$, AIC; BIC, and adjusted $R^2$

The training error will decrease as more variables are included in the model, but the test error may not. Therefore, training set RSS and training set $R^2$ cannot be used to select from among a set of models with different numbers of variables. However, a number of techniques for *adjusting* the training error for the model size are available. These approaches can be used to select among a set of models with different numbers of variables. We now consider four such approaches: $C_p$, *Akaike information criterion (AIC), *Bayesian information criterion (BIC), and adjusted $R^2$, 

For a fitted least squares model containing $d$ predictors, the $C_p$ estimate of test MSE is computed using the equation:

$$C_p = \frac{1}{n}(RSS + 2d\hat{\sigma}^2)$$

where $\hat{sigma}^2$ is an estimate of the variance of the error $\epsilon$ associated with each response measurement. Essentially, the $C_p$ statistic adds a penalty of $2d\hat{sigma}^2$ to the training RSS in order to adjust for the fact that the training error tends to underestimate the test error. It is possible to show that if $\hat{sigma}^2$ is an unbiased estimate of $\sigma^2$, then $C_p$ is an unbiased estimate of test MSE. As a consequence, the $C_p$ statistic tends to take on a small value for values with a low test error, so when determining which of a set of models is best, we choose the model with the lowest $C_p$ value.

The AIC criterion is defined for a large class of models fit by maximum likelihood. In the case of least regression where you have Gaussian errors, it is the same thing as maximum likelihood. In this case, AIC is given by 

$$C_p = \frac{1}{n\hat{\sigma}^2}(RSS + 2d\hat{\sigma}^2)$$
Hence for least squares models, $C_p$ and AIC are proportional to each other.

BIC is derived from a Bayesian point of view but ends up looking similar to $C_p$ and AIC as well. FOr least squares model with $d$ predictors, the BIC is, up to irrelevant constants, given by

$$BIC = \frac{1}{n}\left ( RSS + log(n)d\hat{\sigma}^2 \right )$$
Like $C_p, the BIC will tend to take on a small value for a model with a low test error, and so generally we select the model that has the lowest BIC value. Since $log n > 2$ for any $n >7$, the BIC statistic generally places a heavier penalty on models with many variables, and hense results in the selection of smaller models than $C_p$. 

The adjusted $R^2$ statistic is another popular approach for selecting among a set of models that contain different numbers of variables. Recall from Chapter 3 that the usual $R^2$ is defined as $1-RSS/TSS$. Since RSS always decreases as more variables are added to the model, the $R^2$ always increases as more variables are added. For a least squares model with $d$ variables, the adjusted $R^2$ statistic is calculated as

$$\text{Adjusted} R^2 = 1 - \frac{RSS/(n-d-1)}{TSS/(n-1)}$$
Unlike the previous statistics we looked at before, a *large* value of the adjusted $R^2$ indicates a model with a small test error. Maximizing it implies minimizing $RSS/(n-d-1)$. The intuition behind the adjusted $R^2$ is that once all of the correct variables have been included in the model, adding additional noise variables will lead to only a very small decrease in RSS. Since adding noise variables leads to an increase in d, such variables will lead to an increase in $\frac{RSS}{n-d-1}$, which leads to a decrease in the adjusted $R^2$.

Despite its popularity, and even though it is quite intuitive, the adjusted $R^2$ is not as well motivated in statistical theory as AIC, BIC, and $C_p$. All of this statistics can be defined for more general types of models.

## Validation and Cross-Validation

We can compute the validation set error or the cross-validation error for each model under consideration, and then select the model for which the resulting estimated test error is smallest. This procedure has an advantage relative to AIC, BIC, $C_p$, and adjusted $R^2$, in that it provides a direct estimate of the test error, and makes fewer assumptions about the true underlying model. It can also be used in a wider range of model selection tasks, even in cases where it is hard to pinpoint the model degrees of freedom (e.g. the number of predictors in the model) or hard to estimate the error variance $\sigma^2$.

Let's say we have the BIC statistic, the validation set error, and the cross-validation error. Past a certain number of variables, the errors flatten, that is, there isn't much in terms of reducing test error by introducing additional variables. However, we're not sure exactly which model to select. In this setting, we can select a model using the *one-standard-error rule*. We first calculate the standard error of the estimated test MSE for each model size, and then select the smallest model for which the estimated test error is within one standard error of the lowest point on the curve. The rationale here is that if a set of models appear to be more or less equally good, then we might as well choose the simplest model — that is, the model with the smallest number of predictors. 

#Shrinkage Methods

As an alternative, we can fit a model containing all $p$ predictors using a technique that *constrains* or *regularizes* the coefficient estiamtes, or equivalently, that *shrinks* the coefficient estimates towards zero. It turns out that shrinking the coefficient estiamtes can significantly reduce their variance. 

## Ridge Regression

Ridge regression is similar to least squares, except that the coefficients are estimated by minimizing a different quantity than RSS. In particular, the ridge regression coefficient estimates $\hat{\beta}^R$ are the values that minimize

$$\sum_{i=1}^n \left(y_i - \beta_0 - \sum^p_{j=1} \beta_j x_{ij} \right )^2 + \lambda \sum_ {j=1}^p \beta_j^2 = RSS + \lambda \sum_ {j=1}^p \beta_j^2$$

where $\lambda \geq 0$ is a *tuning parameter*, to be determined separately.

As with least squares, ridge regression seeks coefficient estimates that fit the data well, by making the RSS small. However, the second term, $\lambda \sum_ {j=1}^p \beta_j^2$, called a *shrinkage penalty*, is small when $\beta_1,...,\beta_p$ are close to zero, and so it has the effect of *shrinking* the estimates of $\beta_j$ towards zero. The tuning parameter $\lambda$ serves to control the relative impact of these two terms on the regression coefficient estimates. When $\lambda = 0$ the penalty term has no effect, and ridge regression will produce the least square estimates. However, as $\lambda \rightarrow \infty$, the impact of the shrinkage penalty grows, and the ridge regression coefficient estimates will approach zero. Unlike least squares, ridge regression will produce a different set of coefficient estimates, $\hat{\beta}^R_\lambda$ for each value of $\lambda$. Selectiong a good value for it is critical, but we will discuss this later.


The notation $||\beta||_ 2$ denotes the $l_2$ norm of a vector, and is defined as $||\beta||_ 2$ = \sqrt{\sum^p_ {j=1} \beta_j^2}. It measures the distane of $\beta$ from zero. As $\lambda$ increases, the $l_2$ norm of $\hat{\beta}_\lambda^R$ will always decrease. This quantity ranges from 1 (when $\lambda = 0$, in which case the ridge regression coefficient estimate is the same as the least squares estimate, and so their $l_2$ norms are the same) to 0 (when $\lambda = \infty$, in which case the ridge regression coefficient estimate is a vector of zeros, with $l_2$ norm equal to 0)

The standard least squares coefficient estimates are *scale invariant*: multiplying $X_j$ by a constant $c$ simply leads to a scaling of the least squares coefficient estimates by a factor of $1/c$. IN other words, regardless of how the $j$th predictor is scaled, $X_j \hat{\beta}_j$ will remain the same. In contrast, the ridge regression coefficient estimates chan change *substantially* when multiplying a given predictor by a constant. For instance, consider the income variable, which is measured in dollars. One could reasonably have measured income in thousands of dollars, which would result in a reduction in the observed values of income by a factor of 1000. Now due to the sum of squared coefficients term in the ridge regression formulation, such a change will not simply cause the ridge regression estimate for income to change by a factor of 1000. In other words, $X_j \hat{\beta}^R_{j,\lambda}$ will depend not only on the value of $\lambda$ but also on the scaling of the $j$th predictor. 

Therefore, it is best to apply ridge regression after *standardizing the predictors*, using the formula

$$\tilde{x_{ij}} = \frac{x_{ij}}{\sqrt{\frac{1}{n} \sum^n_ {i=1} \left( x_{ij} - \bar{x_j}\right)}}$$
so that they are all on the same scale.

## Why does ridge regression improve over least squares?

Ridge regression’s advantage over least squares is rooted in the bias-variance trade-off . As $\lambda$ increases, the flexibility of the ridge regression fit decreases, leading to decreased variance but increased bias. 

In general, in situations where the relationship between the response and the predictors is close to linear, the least squares estimates will have low bias but may have high variance. This means that a small change in the training data can cause a large change in the least squares coefficient estimates. In particular, when the number of variables p is almost as large as the number of observations n, the least squares estimates will be extremely variable. And if $p > n$, then the least squares estimates do not even have a unique solution, whereas ridge regression can still perform well by trading off a small increase in bias for a large decrease in variance. Hence, ridge regression works best in situations where the least squares estimates have high variance.

Ridge regression also has substantial computational advantages over best subset selection, which requires searching through $2^p$ models. As we discussed previously, even for moderate values of $p$, such a search can be computationally infeasible. In contrast, for any fixed value of $\lambda$, ridge regression only fits a single model, and the model-fitting procedure can be performed quite quickly.

# The Lasso

Ridge regression does have one obvious disadvantage. Unlike best subset, forward stepwise, and backward stepwise selection, which will generally select models that involve just a subset of the variables, ridge regression will include all $p$ predictors in the final model. The penalty will shrink all of the coefficients towards zero, but it will not set any of them exactly to zero unless $\lambda = \infty$.  This may not damage the prediction accuracy of the model but it may hinder its interpretability. 

The *lasso* is a relatively recent alternative to ridge regression that overcomes this disadvantage. The lasso coefficients, $\hat{\beta}^L_\lambda$ minimize the quantity 

$$ RSS + \lambda \sum_{j=1}^p |\beta_j|$$
The difference is that the $\beta^2_j$ term in the ridge regression penalty is replaced by $|\beta_j|$ in the lasso penalty. In statistical parlance, the lasso uses an $l_1$ penalty instead of an $l_2$ penalty. The $l_1$ norm of a coefficient vector $\beta$ is given by $||\beta||_1 = \sum |\beta_j|$.

As with ridge regression, the lasso shrinks the coefficient estimates towards zero. However, in the case of the lasso, the $l_1$ penalty has the effect of forcing some of the coefficient estmiates to be exactly equal to zero when the tuning parameter $\lambda$ is sufficiently large. Hence, much like best subset selection, the lasso performs *variable selection*. As a result, models generated from the lasso are generally much easier to interpret than those produced by ridge regression. We say that the lasso yields *sparse* models - that is, models that involve only a subset of the variables. As in ridge regression, a good value for $\lambda$ is critical. 

## Comparing the Lasso and Ridge Regression

It is clear that the lasso has a major advantage over ridge regression in that it produces simpler and more interpretable models that involve only a subset of the predictors. However, which method leads to better prediction accuracy?

In general, one might expect the lasso to perform better in a setting where a relatively small number of predictors have substantial coefficients and the remaining predicotrs have coefficients that are very small or that qual to zero. Ridge regression will perform better when the response is a function of many predictors, all with coefficients of roughly equal size. However, the number of predictors that is related to the response is never known *a priori* for real data sets. A technique such as cross-validation can be used in order to determine which approach is better on a particular data set.

As with ridge regression, when the least squares estimates have excessively high variance, the lasso solution can yield a reduction in variance at the expense of a small increase in bias, and consequently can generate more accurate predictions. Unlike ridge regression, the lasso performs variable selection and hence results in models that are easier to interpret. 

## Selecting the Tuning Parameter

Cross-validation provides a simple way to tackle this problem.

We choose a grid of $\lambda$ values and compute the cross-validation error for each value of $\lambda$. We then select the tuning parameter value for which the cross-validation error is smallest. Finally, the model is re-fit using all of the available observations and the selected value of the tuning parameter. 

# Dimension Reduction Methods

The methods we have discussed so far in this chapter have controlled variance in two different ways, ither by using a subset of the original variables, or by shrinking their coefficients toward zero. All of these methods are defined using the original predictors $X_1,X_2,...,X_p$. We now explore a class of approaches that *transform* the predictors and then fit a least squares model using the transformed variables. We will refer to these techniques as *dimension reduction* methods.

Let $Z_1,Z_2,...,Z_M$ represent $M<p$ linear combinations of our original prectors. That is,

$$Z_m = \sum_{j=1}^p \phi_{jm}X_j$$

for some constrants $\phi_{1m},\phi_{2m},...,\phi{pm}$, $m=1,...,M$. We can then fit the linear regression model

$$y_i = \theta_0 + \sum_{m=1}^M \theta_m z_{im} + \epsilon_i, i=1,...,n$$

using least squares.

The term *dimension reduction* comes from the fact that this approach reduces the problem of estimating the $p+1$ coefficients $\beta_0,\beta_1,...,\beta_p$ to the simpler problem of estimating the $M+1$ coefficients, $\theta_0,\theta_1,...,\theta_M$, where $M_p$. In other words, the dimension of the problem has been reduced from $p+1$ to $M+1$.

All dimension reduction methods work in two steps. First, the transformed predictors are obtained. Second, the model is fit using these $M$ predictors. However, the choice of $Z_1,...,Z_M$, or equivalently, the selection of the $\phi_{jm}$'s can be achieved in different ways. In this chapter, we will consider two approaches for this task: *principal components* and *partial least squares*.

## Principal Components Regression

*Principal components analysis* (PSA) is a popular approach for deriving a low-dimensional set of features from a large set of variables. PCA is discussed in greater detail as a tool for *unsupervised learning* in Chapter 10. Here we describe its use as a dimension reduction technique for regression.

** An overview of PCA** 

PCA is a technique for reducing the dimension of a $n \times p$ data matrix $\mathbf{X}$. THe *first principal component* direction of the data is that along which the observations *vary the most*. Another interpretation is that the first principal component vector defines the line that is *as close as possible* to the data. 

The second principal component $Z_2$ is a linear combination of the variables that is uncorrelated with $Z_1$, and has largest variance subject to this constraint. This is equivalent to the condition that they must be orthogonal. With $p$ predictors we would need to consider additional components for construction. THey would sucessively maximize variance, subject to the constraint of being uncorrelated with the preceding components.

## The Principal Components Regression Approach

The *principal components regression* approach involves constructing the first $M$ principal components, $Z_1,...,Z_M$, and then using these components as the predictors in a linear regression model that is fit using least squares. The key idea is that often a small number of principal ocmponents suffice to explain most of the variability in the data, as well as the relationship with the response. In other words, we assume that *the directions in which $X_1,...,X_p$ show the most variation are the directions that are associated with $Y$*. WHile the assumption is not guaranteed to be true, it often turns out to be a reasonable enough approximation to give good results.

If the assumption underlying PCR holds, then fitting a least squares model to $Z_1,..,Z_M$ will lead to better results than fitting said model to the original predictors because most or all of the information will be contained in the new predictors, and by estimating only $M \ll p$ coefficients we can mitigate overfitting.

We note that even though PCR provides a simple way to perform regression using $M<p$ predictors, it is *not* a feature selection method. This is because each of the $M$ principal components used in the regression is a linear ocmbination of all $p$ of the *original* features. 

When performing PCR we generally recommend *standardizing* each predictor prior to generating the principal components. This standardization ensures that all variables are on the same scale. In the absence of standardization, the high-variance variables will tend to play a larger role in the principal components obtained, and the scale on which the variables are measured will ultimately have an effect on the final PCR model.

##Partial Least Squares

We now present *partial least squares*, a *supervised* alternative to PCR. Like PCR, PLS is a dimension reduction method, which first identifies a set of features $Z_1,...,Z_M$ that are linear combinations of the original features, and then fits a linear model via least squares using these $M$ new features. Unlike PCR, PLS identifies these new features in a supervised way - that is, it makes use of the repsonse $Y$ in order to identify new features that not only approximate the old features well, but also *are related to the response*

We now describe how the first PLS direction is computed. After standardizing the $p$ predictors, PLS computes the first direction $Z_1$ by setting each $\phi_{j1}$ equal to the coefficient from the simple linear regression of $Y$ onto $X_j$. One can show that this coefficient is proportional to the correlation between $Y$ and $X_j$. Hence in computing $Z_1 = \sum_{j=1}^+ \phi_{j1} X_j$, PLS places the highest weight on the variables that are most strongly related to the response. To identify the second PLS direction we first *adjust* each of the variables for $Z_1$, by regressing each variable on $Z_1$ and taking *residuals*. These residuals can be interpreted as the remaining information that has not been explained by the first PLS direction. We then compute $Z_2$ using this *orthogonalized* data in exactly the same fashion as $Z_1$ was computed based on the original data. This iterative approach can be repeated $M$ times to identify multiple PLS components $Z_1,...,Z_M$, Finally, at the end of this procedure, we use least squares to fit a linear model to predict $Y$ using $Z_1,...,Z_M$ in exactly the same fashion as for PCR.

#Lab 1 - Subset Selection Methods

** Best Subset Selection** 

```{r}
library(ISLR)
fix(Hitters)
names(Hitters)
dim(Hitters)
sum(is.na(Hitters))   #how many NA
Hitters=na.omit(Hitters) #remove them
dim(Hitters)
sum(is.na(Hitters)) #now you have 0
```

```{r}
library(leaps)
regfit.full<-regsubsets(Salary~.,Hitters)
summary(regfit.full)
regfit.full<-regsubsets(Salary~.,Hitters,nvmax=19)
reg.summary<-summary(regfit.full)
names(reg.summary)
reg.summary$rsq
par(mfrow=c(2,2))
plot(reg.summary$rss,xlab="Number of variables", ylab="RSS")
plot(reg.summary$adjr2,xlab="Number of variables",ylab="Adjusted Rsq")
which.max(reg.summary$adjr2)
points(11,reg.summary$adjr2[11],col="red",cex=2,pch=20) #colors the 11th point in the previous adjr2 plot
plot(reg.summary$cp,xlab="Number of variables", ylab="Cp")
which.min(reg.summary$cp)
points(10,reg.summary$cp[10],col="red",cex=2,pch=20)
plot(reg.summary$bic,xlab="Number of variables", ylab="BIC")
which.min(reg.summary$bic)
points(6,reg.summary$bic[6],col="red",cex=2,pch=20)
```

```{r}
plot(regfit.full,scale="r2")
plot(regfit.full,scale="adjr2")
plot(regfit.full,scale="Cp")
plot(regfit.full,scale="bic")
```

```{r}
coef(regfit.full,6)
```

**Forward and Backward Stepwise Selection**

```{r}
regfit.fwd <- regsubsets(Salary~.,data=Hitters,nvmax=19,method="forward")
summary(regfit.fwd)
regfit.bwd <- regsubsets(Salary~.,data=Hitters,nvmax=19,method="backward")
summary(regfit.bwd)
```

**Choosing among models using the validation set approach and cross-validation**

```{r}
set.seed(1)
train=sample(c(TRUE,FALSE),nrow(Hitters),rep=TRUE)
test=(!train)
```
```{r}
regfit.best=regsubsets(Salary~.,data=Hitters[train,],nvmax=19)
test.mat=model.matrix(Salary~.,data=Hitters[test,])
val.errors=rep(NA,19)
for (i in 1:19) {
  coefi<-coef(regfit.best,id=i)
  pred<-test.mat[,names(coefi)]%*%coefi
  val.errors[i]<-mean((Hitters$Salary[test]-pred)^2)
}
val.errors
which.min(val.errors)
coef(regfit.best,10)
```

```{r}
predict.regsubsets=function(object,newdata,id,...){
  form=as.formula(object$call[[2]])
  mat=model.matrix(form,newdata)
  coefi=coef(object,id=id)
  xvars=names(coefi)
  mat[,xvars]%*%coefi
}

```

```{r}
k=10
set.seed(1)
folds=sample(1:k,nrow(Hitters),replace=TRUE)
cv.errors=matrix(NA,k,19,dimnames=list(NULL,paste(1:19)))
for (j in 1:k) {
  best.fit<-regsubsets(Salary~.,data=Hitters[folds!=j,],nvmax=19)
  for (i in 1:19) {
    pred<-predict(best.fit,Hitters[folds==j,],id=i)
    cv.errors[j,i]=mean((Hitters$Salary[folds==j]-pred)^2)
  }
}

mean.cv.errors=apply(cv.errors,2,mean)
mean.cv.errors
par(mfrow=c(1,1))
plot(mean.cv.errors,type='b')
```


#Lab 2: Ridge Regression and the Lasso

If alpha = 0 then a ridge regression is fit, if alpha = 1 then a lasso model is fit.

```{r}
x=model.matrix(Salary~.,Hitters)[,-1]
y=Hitters$Salary
```

##Ridge Regression

```{r}
library(glmnet)
grid=10^seq(10,-2,length=100)
ridge.mod=glmnet(x,y,alpha=0,lambda=grid)
```

By default the *glmnet()* function performs ridge regression for an automatically selected range of $\lambda$ values. However, here we have chosen to implement the function over a grid of values ranging from $\lambda = 10^10$ to $\lambda=10^{-2}$. Note that by default the *glmnet()* function standardizes the variables so that they are on the same scale. To turn this off use the argument *standardize=FALSE*.

```{r}
dim(coef(ridge.mod))
```

We expect the coefficient estimates to be much smaller, in terms of $l_2$ norm, when a large value of $\lambda$ is used, as compared to when a small value is used.

```{r}
ridge.mod$lambda[50]
coef(ridge.mod)[,50]
sqrt(sum(coef(ridge.mod)[-1,50]^2))
```

```{r}
ridge.mod$lambda[60]
coef(ridge.mod)[,60]
sqrt(sum(coef(ridge.mod)[-1,60]^2))
```

Note the much larger $l_2$ norm of the coefficients associated with this smaller value of $\lambda$.

```{r}
predict(ridge.mod,s=50,type="coefficients")[1:20,]
```

We'll now split the data into a training set and a test set in order to estimate the test error of ridge regression and the lasso.

```{r}
set.seed(1)
train=sample(1:nrow(x),nrow(x)/2)
test=(-train)
y.test=y[test]
```


```{r}
ridge.mod =glmnet(x[train,],y[train],alpha=0,lambda=grid,thresh=1e-12)
ridge.pred=predict(ridge.mod,s=4,newx=x[test,])
mean((ridge.pred-y.test)^2)
```

```{r}
ridge.pred=predict(ridge.mod,s=1e10,newx=x[test,]) #lambda = 1e10
mean((ridge.pred-y.test)^2) #test set MSE
```

We now check whether there is any benefit to performing ridge regression with $\lambda=4$ instead of just performing least squares regression. Recall that least squares is simply ridge regression with $\lambda=0$.

```{r}
ridge.pred=predict(ridge.mod,s=0,newx=x[test,],exact=T) #lambda = 0
mean((ridge.pred-y.test)^2) #test set MSE
```

Instead of arbitrarily choosing $\lambda=4$, it would be better to use cross-validation to choose the tuning parameter.

```{r}
set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=0)
plot(cv.out)
bestlam=cv.out$lambda.min
bestlam
```

Finally, we refit our ridge regression model on the full data set, using the value of $\lambda$ chosen by cross-validation, and examine the coefficient estimates.

```{r}
out=glmnet(x,y,alpha=0)
predict(out,type='coefficients',s=bestlam)[1:20,]

```

##The Lasso

```{r}
lasso.mod=glmnet(x[train,],y[train],alpha=1,lambda=grid)
plot(lasso.mod)
```

We can see from the coefficient plot that depending on the choice of tuning parameter, some of the coefficients will be exactly equal to zero. We now perform cross-validation and compute the associated test error.

```{r}
set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=1)
plot(cv.out)
bestlam=cv.out$lambda.min
lasso.pred=predict(lasso.mod,s=bestlam,newx=x[test,])
mean((lasso.pred-y.test)^2)
```

This is much better than the test MSE of the null model and of least squares, and very similar to the ridge regression with $\lambda$ chosen by cross-validation. However, the lasso has an advantage in that the resulting coefficient estiamtes are sparse. Here we see that the lasso model with $\lambda$ chosen by cross-validation contains only seven variables

```{r}
out=glmnet(x,y,alpha=1,lambda=grid)
lasso.coef=predict(out,type="coefficients",s=bestlam)[1:20,]
lasso.coef
lasso.coef[lasso.coef!=0]
```

#Lab 3: PCR and PLS Regression
## Principal Components Regression

```{r}
library(pls)
set.seed(2)
pcr.fit=pcr(Salary~.,data=Hitters,scale=TRUE,validation="CV")
```

Setting *scale=TRUE* has the effect of *standardizing* each predictor, according to the equation seen earlier, prior to generating the principal components so that the scale on which each variable is measured will not have an effect. Setting *validation="CV"* causes *pcr()* to perform 10-fold cross-validation for each possible value of $M$, the number of principal components used

.
```{r}
summary(pcr.fit)
```

Note that this is the *root mean squared error*. One cal also plot the cross-validation scores using the *validationplot()* function. Using *val.type="MSEP"* will cause the cross-validation MSE to be plotted.

```{r}
validationplot(pcr.fit,val.type = "MSEP")
```

Note that the lowest MSE occurs when $M=16$, which is barely better than performing just least squares regression. However, the plot shows that using a single component in the model gives us roughly the same cross-validation error. This suggests that a model that uses just a small number of components might suffice.

We now perform PCR on the training data and evaluate its test set performance

```{r}
set.seed(1)
pcr.fit=pcr(Salary~.,data=Hitters,subset=train,scale=TRUE,validation="CV")
validationplot(pcr.fit,val.type = "MSEP")
```

We compute the test MSE as follows.

```{r}
pcr.pred=predict(pcr.fit,x[test,],ncomp=7) #validation plot indicates lowest MSE with 7 components
mean((pcr.pred-y.test)^2)
```

This test set MSE is competitive with the results obtained using ridge regression and the lasso. However, as a result of the way PCR is implemented, the final model is more difficult to interpret because it does not perform ny kind of variable selection or even directly produce coefficient estimates.

Finally we fit PCR on the full data set.

```{r}
pcr.fit=pcr(y~x,scale=TRUE,ncomp=7)
summary(pcr.fit)
```

##Partial least squares

```{r}
set.seed(1)
pls.fit=plsr(Salary~.,data=Hitters,subset=train,scale=TRUE,validation="CV")
summary(pls.fit)
```

```{r}
pls.pred=predict(pls.fit,x[test,],ncomp=2) #lowest MSE with 2 components
mean((pls.pred-y.test)^2)
```

```{r}
pls.fit=plsr(Salary~.,data=Hitters,scale=TRUE,ncomp=2)
summary(pls.fit)
```

Notice that the percentage of variance in **Salary** that the two-component PLS fit expalins, 46.40%, is almost as much as that explained using the final seven-component model PCR fit, 46.69%. This is because PCR only attempts to maximize the amount of variance explained in the predictrs, while PLS searches for directions that explain variance in both the predictors and the response.
